{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e490d2",
   "metadata": {},
   "source": [
    "# Small Image captioning transformer\n",
    "\n",
    " Image captioning (describes the context of an image) using a processing efficient architecture.\n",
    " - small CNN or VisionTransformer for image context\n",
    " - encoder - decoder with text and vision information that generates in autoregressive manner the image captioning\n",
    "\n",
    "Author: fvilmos\n",
    "\n",
    "See references:\n",
    "- Attention Is All You Need - https://arxiv.org/abs/1706.03762\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda27d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "from thop import profile\n",
    "from utils.mardown_display import mm\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.decoder import ImageCaptioner, generate_caption\n",
    "from utils.vision_encoder import CNNEncoder, ViTEncoder, MobileNetV2Encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf8a0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=80\n",
    "IN_CHANNELS = 3\n",
    "IMG_SIZE = 224\n",
    "EPOCS = 20\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY=1e-4\n",
    "LOG_STEP = 100\n",
    "MAX_LEN= 100\n",
    "MAX_GEN = 50\n",
    "\n",
    "save_path = \"./sic_model.pth\"\n",
    "root_dir = '../../datasets/_coco/train2017'\n",
    "annotation_file = '../../datasets/_coco/captions_train2017.json'\n",
    "\n",
    "# model evaluation\n",
    "model_path = \"./sic_model.pth\"\n",
    "root_dir_val = '../../datasets/_coco/val2017'\n",
    "annotation_file_val = '../../datasets/_coco/captions_val2017.json'\n",
    "\n",
    "# model test\n",
    "root_dir_tst = '../../datasets/_coco/test2017'\n",
    "annotation_file_tst = '../../datasets/_coco/captions_test2017.json'\n",
    "\n",
    "workers = 0\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"available device ==>\", device, \"\\n\")\n",
    "\n",
    "# test img list\n",
    "img_list = glob.glob(root_dir_tst + '/*.jpg')\n",
    "print (\"test images:\", len(img_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 building blocks\n",
    "mm(\"\"\"\n",
    "   flowchart LR;\n",
    "   subgraph imagein [Image processing];\n",
    "      A([\"input\\nimage\"])--|3xwxh|-->B[\"VisionEncoder\"];\n",
    "   end;\n",
    "   subgraph transformer [transformer blocks];\n",
    "      B---->C[\"Cross attention\"];\n",
    "   end;\n",
    "   subgraph textin [\"text processing\"]\n",
    "      D[\"Masked self-attention\"]-->C\n",
    "      E[\"Word embedding \\n positional embedding\"]-->D\n",
    "      G([\"start sequence\"])-->E\n",
    "   end\n",
    "   C --> F[\"Decoding\"]\n",
    "   \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2b427",
   "metadata": {},
   "source": [
    "# Vocabulary generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(json_path, threshold=3):\n",
    "    coco = COCO(json_path)\n",
    "    vocab = Vocabulary()\n",
    "    counter = {}\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = vocab.custom_word_tokenize(caption)\n",
    "        for word in tokens:\n",
    "            counter[word] = counter.get(word, 0) + 1\n",
    "\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    for word in words:\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "voc = build_vocab(annotation_file, 4)\n",
    "print (\"voc len ==>\",len(voc))\n",
    "\n",
    "# export voc\n",
    "voc.export_vocabulary(\"./voc.json\")\n",
    "\n",
    "print ('vocabulary exported!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a54e92-e25c-4bec-8614-0ff3ae8e0466",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoCaptionDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Helper to get COCO data and dataloder\n",
    "    \"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ann_id = self.ids[index]\n",
    "        caption = self.coco.anns[ann_id]['caption']\n",
    "        img_id = self.coco.anns[ann_id]['image_id']\n",
    "        path = self.coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        tokens = self.vocab.custom_word_tokenize(str(caption))\n",
    "        caption = []\n",
    "        caption.append(self.vocab('<start>'))\n",
    "        caption.extend([self.vocab(token) for token in tokens])\n",
    "        caption.append(self.vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "def collate_fn(data):\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]\n",
    "    return images, targets, lengths\n",
    "\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    coco = CocoCaptionDataset(root=root,\n",
    "                              json=json,\n",
    "                              vocab=vocab,\n",
    "                              transform=transform)\n",
    "    \n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "\n",
    "data_loader = get_loader(root=root_dir, json=annotation_file,vocab=voc,transform=transform,batch_size=BATCH_SIZE,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ebe241-3242-47ff-89af-cbaf9f76fcbe",
   "metadata": {},
   "source": [
    "## Model and complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea28511-7a0c-45f3-aca4-8dc6c870486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ImageCaptioner(vocab_size=len(voc),\n",
    "                       dim=256, \n",
    "                       num_heads=4,\n",
    "                       num_layers=2, \n",
    "                       vis_out_dimension=1280, \n",
    "                       vis_hxw_out=49,\n",
    "                       max_len = MAX_LEN,\n",
    "                       VisionEncoder=MobileNetV2Encoder).to(device)\n",
    "\n",
    "# model = ImageCaptioner(vocab_size=len(voc),\n",
    "#                        dim=768, \n",
    "#                        num_heads=8,\n",
    "#                        num_layers=4, \n",
    "#                        vis_out_dimension=512, \n",
    "#                        vis_hxw_out=49,\n",
    "#                        max_len = MAX_LEN,\n",
    "#                        VisionEncoder=CNNEncoder).to(device)\n",
    "\n",
    "# # ViT backbone\n",
    "# model = ImageCaptioner(vocab_size=len(voc),\n",
    "#                        dim=768, \n",
    "#                        num_heads=8,\n",
    "#                        num_layers=4, \n",
    "#                        vis_out_dimension=768, \n",
    "#                        vis_hxw_out=50, # 49 + CLS\n",
    "#                        max_len = MAX_LEN,\n",
    "#                        VisionEncoder=ViTEncoder).to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=3e-4,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# get model params and complexity\n",
    "voc_size = len(voc)\n",
    "dummy_images = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "dummy_captions = torch.randint(0, voc_size, (1, MAX_LEN - 1)).long().to(device)\n",
    "\n",
    "# Profile the model using thop\n",
    "macs, params = profile(model, inputs=(dummy_images, dummy_captions))\n",
    "\n",
    "print(f\"Total MACs: {macs / 1e9:.2f} G, FLOPS: {2*macs / 1e9:.2f} G\")\n",
    "print(f\"Total Parameters: {params / 1e6:.2f} M\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc5840-a39e-43bf-bbf6-d357da06e919",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = 1e+9\n",
    "def save_model(in_model,path):\n",
    "    torch.save(in_model.state_dict(),path)\n",
    "\n",
    "for epoch in range(EPOCS):\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        model.train()\n",
    "        model.vision.eval()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        targets = captions[:, 1:].to(device)\n",
    "\n",
    "        outputs = model(images, captions[:, :-1])\n",
    "        \n",
    "        loss = torch.nn.functional.cross_entropy(outputs.reshape(-1, len(voc)), targets.reshape(-1), ignore_index=voc('<pad>'))\n",
    "\n",
    "        if best_loss > loss.item():\n",
    "            #save model\n",
    "            save_model(model,save_path)\n",
    "            best_loss = loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % LOG_STEP == 0:\n",
    "            print(f'Epoch [{epoch+1}/{EPOCS}], Step [{i}/{len(data_loader)}],', f'Loss: {loss.item():.4f}, Perplexity: {torch.exp(loss).item():.4f}')\n",
    "\n",
    "    # genrate 3 samples, as a test\n",
    "    for _ in range(3):\n",
    "        val = np.random.randint(len(img_list))\n",
    "        image_o = Image.open(img_list[val]).convert('RGB')\n",
    "        image = transform(image_o.copy()).to(device)\n",
    "\n",
    "        cap, att = generate_caption(model,image,voc,MAX_GEN, return_att=True)\n",
    "        \n",
    "        # test attention\n",
    "        # smash all heads information to a single layer\n",
    "        token_idx = 3\n",
    "        att = att.mean(dim=1)\n",
    "        att = att[0,token_idx]\n",
    "\n",
    "        print (\"caption:\", cap)\n",
    "        plt.imshow(image_o)\n",
    "        plt.show()\n",
    "\n",
    "    torch.save(model.state_dict(), \"sic_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e860b73e-c9a3-4422-8f47-1d75c99d8d92",
   "metadata": {},
   "source": [
    "# Test captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c817e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print (\"voc len:\", len(voc))\n",
    "except:\n",
    "    voc = Vocabulary()\n",
    "    voc.import_vocabulary('./voc.json')\n",
    "    print (\"voc len:\", len(voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72a2f2-5bc9-44f4-b8f0-4f23b6b102e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = model = ImageCaptioner(vocab_size=len(voc),\n",
    "                       dim=256, \n",
    "                       num_heads=4,\n",
    "                       num_layers=2, \n",
    "                       vis_out_dimension=1280, \n",
    "                       vis_hxw_out=49,\n",
    "                       max_len = MAX_LEN,\n",
    "                       VisionEncoder=MobileNetV2Encoder).to(device)\n",
    "\n",
    "test_model.load_state_dict(torch.load('./sic_model.pth', map_location=device), strict=False)\n",
    "\n",
    "transform_nn = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for _ in range(3):\n",
    "        val = np.random.randint(len(img_list))\n",
    "        image_o = Image.open(img_list[val]).convert('RGB')\n",
    "        \n",
    "        image = transform_nn(image_o.copy()).to(device)\n",
    "        \n",
    "        cap = generate_caption(test_model,image,voc,50, return_att=False)\n",
    "        \n",
    "        print (\"caption:\",\" \".join(cap[0]))\n",
    "        plt.imshow(image_o)\n",
    "        plt.axis('off')\n",
    "        plt.title(\" \".join(cap[0][1:-2]))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420ab77d-5a0f-4d54-a595-ea25bd4de41e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
